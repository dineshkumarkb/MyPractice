{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Hand-written Digits Classification Experiment\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Demo for using SageMaker Experiment Management ( Version Alpha)\n",
    "\n",
    "This demo shows how you can use SageMaker Experiment Management Python SDK to organize, track, compare, and evaluate your machine learning (ML) model training experiments. \n",
    "\n",
    "You can track artifacts for hundreds and thousands of experiments, including data sets, algorithms, hyper-parameters, and metrics. Experiments executed on SageMaker will be automatically tracked. In addition, you can use the APIs to track experiments executed outside SageMaker e.g. models trained locally in your notebooks. You can also track artifacts for additional steps within an ML workflow that come before/after model training e.g. data pre-processing or post-training model evaluation. \n",
    "\n",
    "The APIs also let you query and compare experiments to pick the best performing models for your business use case.\n",
    "\n",
    "Now we will demonstrate these capabilities through an MNIST hand written digits classification example. The experiment will be organized as follow:\n",
    "\n",
    "1. Download and prepare the mnist dataset\n",
    "\n",
    "\n",
    "2. Train 2-layer Multi Layer Perceptron (MLP) network.\n",
    "\n",
    "\n",
    "3. Tune the number of hidden units in the network. Use SageMaker Experiment Management Python SDK APIs to track parameters and results.\n",
    "\n",
    "\n",
    "4. Train a Convolutional Neural Network (CNN) model on SageMaker to compare its performance against the MLP based approach. For this training run executed using SageMaker estimator, the experiment parameters are automatically tracked  - no additional instrumentation required.\n",
    "\n",
    "\n",
    "5. Finally, use  the analytics capabilities of Python SDK to visualize and compare the performance of all the model versions generated in previous steps.\n",
    "\n",
    "Note that the ML framework we will be using throughout the experiment is `Pytroch` and `Scikit-learn`. So please switch the notebook kernel to `conda_pytorch_p36` if you haven't done so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add boto service model\n",
    "!aws configure add-model --service-model file://./source/build/model/sagemaker-2017-07-24.normal.json --service-name sagemaker\n",
    "!cp ./source/build/model/sagemaker-2017-07-24.paginators.json ~/.aws/models/sagemaker/2017-07-24/paginators.json\n",
    "!cp ./source/build/model/sagemaker-2017-07-24.waiters-2.json ~/.aws/models/sagemaker/2017-07-24/waiters-2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.experiments.experiment import Experiment\n",
    "from sagemaker.experiments.trial import Trial\n",
    "from sagemaker.experiments.trial_component import TrialComponent\n",
    "from sagemaker.analytics import TrialAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = boto3.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sagemaker client\n",
    "sm = sess.client('sagemaker', region_name=sess.region_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a S3 bucket to hold data\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">You will need to have permissions to create bucket.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a s3 bucket to hold data\n",
    "account_id = sess.client('sts').get_caller_identity()[\"Account\"]\n",
    "bucket = 'sagemaker-experiments-alpha-{}-{}'.format(account_id, sess.region_name)\n",
    "prefix = 'mnist'\n",
    "\n",
    "# list buckets to ensure no bucket with the same name gets created\n",
    "s3_client = sess.client('s3')\n",
    "response = s3_client.list_buckets()\n",
    "buckets = [bucket['Name'] for bucket in response['Buckets']]\n",
    "\n",
    "if bucket in buckets:\n",
    "    print(\"{} already exists.\".format(bucket))\n",
    "else:\n",
    "    # create the bucket\n",
    "    s3_client.create_bucket(Bucket=bucket, CreateBucketConfiguration={'LocationConstraint': sess.region_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, Transform and upload Dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download the dataset this will not only download data to ./mnist folder, but also load and transform them\n",
    "train_set = datasets.MNIST('mnist', train=True, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))]), \n",
    "    download=True)\n",
    "                           \n",
    "test_set = datasets.MNIST('mnist', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))]),\n",
    "    download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_set.data[2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the data to s3\n",
    "inputs = sagemaker.Session().upload_data(path='mnist', bucket=bucket, key_prefix=prefix)\n",
    "print('input spec: {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Set up the Experiment\n",
    "Create an experiment to track all the model training iterations. Experiments are a great way to organize your data science work. You can create experiments to organize all your model development work for : [1] a business use case you are addressing (e.g. create experiment named “customer churn prediction”), or [2] a data science team that owns the experiment (e.g. create experiment named “marketing analytics experiment”), or  [3] a specific data science and ML project. Think of its as a “folder” for organizing your “files”.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an experiment to track training jobs for creating a mnist classifier.\n",
    "mnist_experiment = Experiment.create(\n",
    "    experiment_name=\"mnist-digits-classification\", \n",
    "    description=\"Classification of mnist hand-written digits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Track Experiment\n",
    "Track each of the model training iterations as “trials” within the experiment.\n",
    "<hr>\n",
    "\n",
    "### Create trials for training multiple versions of  an mlp model, each with a different value for number of hidden units. Track the experiment artifacts using the logging api offered by experiment management python sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(\n",
    "    train_set,\n",
    "    test_set,\n",
    "    tracker,\n",
    "    num_layer=2, \n",
    "    num_hidden=64, \n",
    "    batch_size=128, \n",
    "    lr=0.003, \n",
    "    optimizer='adam',\n",
    "    max_iter=300,\n",
    "    random_state=42,\n",
    "    train_sample_size=3000,\n",
    "):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    # log the dataset\n",
    "    tracker.log_input(\"mlp_training_dataset\", inputs)\n",
    "    \n",
    "    parameters = {\n",
    "        \"num_layer\": num_layer,\n",
    "        \"num_hidden\": num_hidden,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"lr\": lr,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"max_iter\": max_iter,\n",
    "        \"train_sample_size\": train_sample_size,\n",
    "        \"random_state\": random_state,\n",
    "        \n",
    "    }\n",
    "    # log all the parameters\n",
    "    tracker.log_parameters(parameters)\n",
    "    \n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=[num_hidden]*num_layer, \n",
    "        batch_size=batch_size,\n",
    "        solver=optimizer,\n",
    "        learning_rate_init=lr,\n",
    "        random_state=random_state,\n",
    "        max_iter=max_iter,\n",
    "    )\n",
    "    \n",
    "    # for demo purpose we sample train set to reduce training time\n",
    "    np.random.seed(1)\n",
    "    sample_indices = np.random.choice(np.arange(train_set.data.shape[0]), size=train_sample_size, replace=False)\n",
    "    X_train, y_train = train_set.data[sample_indices], train_set.targets[sample_indices]\n",
    "    \n",
    "    X_train = X_train.numpy().flatten().reshape(-1, 28*28)\n",
    "    y_train = y_train.numpy()\n",
    "    \n",
    "    mlp.fit(X_train, y_train)\n",
    "        \n",
    "    train_acc = mlp.score(X_train, y_train)\n",
    "    \n",
    "    X_test, y_test = test_set.data, test_set.targets\n",
    "    X_test = test_set.data.numpy().flatten().reshape(-1, 28*28)\n",
    "    y_test =  test_set.targets.numpy()\n",
    "    \n",
    "    test_acc = mlp.score(X_test, y_test)\n",
    "        \n",
    "    return [num_hidden, train_acc*100, test_acc*100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tune the number of hidden units in network and record the accuracy for each local mlp training job. The Alpha version of SageMaker Experiment Management Python SDK does not support APIS for tracking training metrics. This  capability will be added when we make Experiment Management generally available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = [4, 10, 24, 32, 64, 86, 128, 256]\n",
    "mlp_training_results = np.zeros((len(hiddens), 3))\n",
    "\n",
    "for i, hidden in enumerate(hiddens):\n",
    "    print(f\"Training: {hidden} hidden units ...\")\n",
    "    trial = mnist_experiment.create_trial(trial_name=f\"local-mlp-training-job-{int(time.time())}\")\n",
    "    with trial.create_tracker(component_name=\"training\") as mlp_training_tracker:\n",
    "        accs = train_mlp(train_set, test_set, num_hidden=hidden, tracker=mlp_training_tracker)\n",
    "        mlp_training_results[i] = accs\n",
    "    print(f\"Done: {hidden} hidden units.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training a CNN using SageMaker estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few ways to log and track your training job executed on sagemaker.\n",
    "\n",
    "1. You can simply provide the experiment name, and a trial will be automatically created in this experiment with the same name as the training job.\n",
    "\n",
    "\n",
    "2.\tIf you didn't provide the experiment name, you can still go back, and move this SageMaker training job to any experiment of your choice.\n",
    "\n",
    "In this example, we supply experiment name as \"mnist-digits-classification\" to automatically track this training run in the ongoing experiment.\n",
    "\n",
    "The training on SageMaker takes few minutes to complete ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# just like how you kick-off a training job on SageMaker before, without any additional instrumentation, \n",
    "# your training job running on SageMaker is automatically tracked.\n",
    "estimator = PyTorch(entry_point='mnist.py',\n",
    "                    role=role,\n",
    "                    sagemaker_session=sagemaker.Session(sagemaker_client=sm),\n",
    "                    framework_version='1.1.0',\n",
    "                    train_instance_count=2,\n",
    "                    train_instance_type='ml.c4.xlarge',\n",
    "                    source_dir='./source',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 6,\n",
    "                        'backend': 'gloo',\n",
    "                        'dropout': 0.3,\n",
    "                        'experiment-name':mnist_experiment.experiment_name,\n",
    "                    },\n",
    "                    metric_definitions=[\n",
    "                        {'Name':'train:loss', 'Regex':'Train Loss: (.*?);'},\n",
    "                        {'Name':'test:loss', 'Regex':'Test Average loss: (.*?),'},\n",
    "                        {'Name':'test:accuracy', 'Regex':'Test Accuracy: (.*?)%;'}\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_training_job_name = \"cnn-training-job-{}\".format(int(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'training': inputs}, job_name=cnn_training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the model training runs for an experiment\n",
    "Now we will use the analytics capabilities of Python SDK to query, visualize and compare the training runs for identifying the best model produced by our experiment\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analytics API returns all the logged metadata in a Pandas data frame for quick and easy analysis in notebook. \n",
    "# You can choose to get all metadata or a subset of it.\n",
    "trial_analytics = TrialAnalytics(\n",
    "    sagemaker_session=Session(sess, sm), \n",
    "    experiment_name=mnist_experiment.experiment_name,\n",
    "    metric_names=['test:accuracy'],\n",
    "    parameter_names=['trial_name', 'num_hidden', 'batch_size', \n",
    "                     'lr', 'num_hidden', 'optimizer', \n",
    "                     'num_layer', 'random_state', 'dropout', \n",
    "                     'InstanceCount', 'InstanceType']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_table = trial_analytics.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we pointed out before, the Alpha version of SageMaker Python SDK doesn’t yet support logging metrics, however to simulate this experience, we will now manually add the testing accuracy for each MLP model version. Once the capability for logging metrics is available, you will be able to use it in the same way as you log other experiment artifacts such as data sets and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_idx in range(analytic_table.shape[0]):\n",
    "    row = analytic_table.iloc[row_idx]\n",
    "    if row['trial_name'].startswith('local-mlp'):\n",
    "        hidden = row['num_hidden']\n",
    "        result_index = np.where(mlp_training_results[:,0] == hidden)[0]\n",
    "        analytic_table.at[row_idx, 'test:accuracy - last'] = mlp_training_results[result_index][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic_table.sort_values(by='test:accuracy - last', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for MLP, when `hidden_units=256`, the 2-layer mlp gives the best performance ~`90%` on testing data set. For the CNN model, the test accuracy is ~`94%`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the hidden units against train/test performances for 2-layer mlp.\n",
    "plt.title(\"2-layer MLP performance\")\n",
    "plt.plot(mlp_training_results[:,0], mlp_training_results[:,1], label=\"train\")\n",
    "plt.plot(mlp_training_results[:,0], mlp_training_results[:,2], label=\"test\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"hidden units\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up experiment entities created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(experiment):\n",
    "    '''Clean up everything in the given experiment object'''\n",
    "    for trial_summary in experiment.list_trials():\n",
    "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
    "        for trial_step in trial.list_trial_components():\n",
    "            trial_step.delete()\n",
    "            # to prevent throttling\n",
    "            time.sleep(1)\n",
    "        trial.delete()\n",
    "    \n",
    "    experiment.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleanup(mnist_experiment)\n",
    "del mnist_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the s3 bucket created \n",
    "<div class=\"alert alert-block alert-warning\">You could delete the bucket if you have the permission to attach the delete s3 bucket permission to the current sagemaker execution role shown below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_client.delete_bucket(Bucket=bucket)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
